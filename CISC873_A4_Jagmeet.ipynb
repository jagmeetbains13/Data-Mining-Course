{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDkL3M1paSSY"
   },
   "source": [
    "# Assignment 4\n",
    "Submitted By: Jagmeet Singh\n",
    "\n",
    "**Problem Formulation**\n",
    "\n",
    "The problem is to predict the price of an Airbnb listing based on the image and the description provided about the listing. Instead of predicting a fixed price for the listing, the problem is formulated to give a range of price defined by three categories namely: beginner, plus, and premium. The data contains listing of areas of Montreal for 2019. \n",
    "\n",
    "Multi-task and multi-modality method will be used to predict the type and price range of the listing.\n",
    "\n",
    "**Data Description**\n",
    "\n",
    "The data is in the form of text and images. Each listing has an image and respective text summary. The training data consists of image and text summary with the label values for type and price range of the listing. The test data consists of image and summary data for each listing.\n",
    "\n",
    "**Methods Used**\n",
    "\n",
    "As the data comes in two different formats, a multi-task model is build. First step includes preprocessing and cleaning the data. For image data is tackled using convolutional neural network and the text data is processed using embedding and GRU/LSTM layers. Attention mechanism is also applied for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vColJO0m7bd",
    "outputId": "93132e11-4418-4d69-a8a5-0bc0e0249bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#Mounting google drive to import data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRunehY8arYF"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm   #progress bar\n",
    "from PIL import Image   #image processing\n",
    "import numpy as np\n",
    "\n",
    "#importing data from drive\n",
    "xy_train_df = pd.read_csv('drive/My Drive/CISC873/A4/train_xy.csv') #training data\n",
    "x_test_df = pd.read_csv('drive/My Drive/CISC873/A4/test_x.csv')     #test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGGjTyMvasWe"
   },
   "outputs": [],
   "source": [
    "#method to load images\n",
    "#loading the images and converting the images to arrays with size(64,64,2)\n",
    "\n",
    "def load_image(file):\n",
    "    try:\n",
    "        file = 'drive/My Drive/CISC873/A4/img_train/'+file\n",
    "        #print(file)\n",
    "        image = Image.open(\n",
    "            file\n",
    "        ).convert('LA').resize((64, 64))   #opening image given in file and converting to tensor(64,64,2)\n",
    "        arr = np.array(image)              #converting to numpy array\n",
    "        #print('Done')\n",
    "    except:\n",
    "        arr = np.zeros((64, 64, 2))        #empty array in case of error in above try\n",
    "    return arr                             #method returns the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "d189ef8fd56143f5b5e5430368e22616",
      "5397529f71ff4bcf90b19644d53d9c77",
      "d674ae5f9e3343a6ac803bee9cbc9ded",
      "0fc99068670e48eba103a8008afbac1f",
      "43901ea7184b4cc9950514df183fc160",
      "e3f2beaab9674dd183c02371ac4f62c3",
      "eb7277c9be9543af81edd38656631c10",
      "c477196754ea48a48ef422ce0e5abb3a"
     ]
    },
    "id": "Sw1Hl4lZnPbQ",
    "outputId": "497eaae3-16c9-47a9-f301-37ef6155f38e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d189ef8fd56143f5b5e5430368e22616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7627.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loading images:\n",
    "x_image = np.array([load_image(i) for i in tqdm(xy_train_df.image)])\n",
    "\n",
    "# loading summary: (force convert some of the non-string cell to string)\n",
    "x_text = xy_train_df.summary.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwccomYLX71w",
    "outputId": "2e67ecc2-c175-4583-d348-27e35ae32ddd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "UDA8oduWXsOf",
    "outputId": "1276eabe-6aa3-4de1-94a3-b07cccb2399f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#displaying one image\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_image[300][:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O2aPL8SNnY5H",
    "outputId": "5b0f9ebc-99f9-44ef-e922-e3af52044567"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7627, 64, 64, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking total shape of training data\n",
    "np.shape(x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67NZzFTfnf3G",
    "outputId": "48d1a046-ecac-4f38-d5ae-7fae15730dfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7627,)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g45LVvctazN2",
    "outputId": "f97df26a-163f-402e-ef3b-38ac8dc5cdd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values for price category 3 [1 0 2]\n",
      "unique values for type category 24 [ 1 17 22 10 18 20  5  2  8  4 23 13 15 16 14 11 19  0 21  3  6 12  7  9]\n"
     ]
    }
   ],
   "source": [
    "#train validation splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# labels:\n",
    "y_price = xy_train_df.price   #label for price category\n",
    "y_type = xy_train_df.type.astype('category').cat.codes   #label for listing category\n",
    "\n",
    "len_price = len(y_price.unique())  #number of unique price categories\n",
    "len_type = len(y_type.unique())    #number of unique listing category\n",
    "\n",
    "#print the unique numbers\n",
    "print('unique values for price category', len_price, y_price.unique())\n",
    "print('unique values for type category', len_type, y_type.unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2R3NDmrX5dt"
   },
   "source": [
    "* The different categories for price are 3 as described.\n",
    "* The different categories for listing type are 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JeLf5ZpX4vQ",
    "outputId": "be313a38-aa3d-4e6a-863a-a1c2a5bf9523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6101, 64, 64, 2)\n",
      "(1526, 64, 64, 2)\n",
      "(6101,)\n",
      "(1526,)\n",
      "(6101,)\n",
      "(1526,)\n"
     ]
    }
   ],
   "source": [
    "#train and validation splitting \n",
    "\n",
    "x_tr_image, x_vl_image, x_tr_text, x_vl_text, y_tr_price, y_vl_price, y_tr_type, y_vl_type = train_test_split(\n",
    "    x_image, \n",
    "    x_text,\n",
    "    y_price,\n",
    "    y_type,\n",
    "    test_size=0.2)\n",
    "\n",
    "#printing shape of each array formed\n",
    "print(np.shape(x_tr_image))\n",
    "print(np.shape(x_vl_image))\n",
    "print(np.shape(y_tr_price))\n",
    "print(np.shape(y_vl_price))\n",
    "print(np.shape(y_tr_type))\n",
    "print(np.shape(y_vl_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFC9G_6ca2o-"
   },
   "outputs": [],
   "source": [
    "#Text data preprocessing\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  #Tokenizer library to give each word a token \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #To convert tokenized array to a 2D array\n",
    "from sklearn.model_selection import train_test_split  # to make train-validation split\n",
    "from pprint import pprint\n",
    "\n",
    "vocab_size = 40000  #defining vocabulary size\n",
    "max_len = 100       #defining max length of the array for each sequence\n",
    "\n",
    "\n",
    "# build vocabulary from training set\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(x_tr_text)  #vocab based on words in training data\n",
    "\n",
    "\n",
    "#Method which assigns integer value to each word in the sequence based on vocabulary \n",
    "#and then transforms into a matrix using pad_sequences\n",
    "\n",
    "#pad_sequences transforms a list of sequences \n",
    "#into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
    "def _preprocess(list_of_text):\n",
    "    return pad_sequences(\n",
    "        tokenizer.texts_to_sequences(list_of_text), #transforms each text in texts to sequence of integers\n",
    "        maxlen=max_len,  #maximum length of all sequences\n",
    "        padding='post',  #specifying padding after sequence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WIwylmlaKpa"
   },
   "source": [
    "* Each sequence (summary text) is converted to array of size 100 based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgH9qiApZ3t3",
    "outputId": "a8242369-0228-43e9-d0fb-49350b1d17f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6101, 100)\n",
      "(1526, 100)\n"
     ]
    }
   ],
   "source": [
    "# padding is done inside: \n",
    "#preprocessing train and validation text data\n",
    "x_tr_text_id = _preprocess(x_tr_text)\n",
    "x_vl_text_id = _preprocess(x_vl_text)\n",
    "\n",
    "#printing shape after preprocessing\n",
    "print(x_tr_text_id.shape)\n",
    "print(x_vl_text_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3SDUOy9bp89",
    "outputId": "492a7a67-8ec7-4418-a76b-f5f585b20f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  72,  137,  128,  156,    5,    3,  116,   77,   38,    4,    1,\n",
       "        496,  524,  830,  348, 1748,  502,   54,   90,    3,  610,  234,\n",
       "       1060, 1344,  409,  753,  165,  335,  753,    8,  216, 1188,    5,\n",
       "          1,  119,   11,  409, 1539,  880,   11, 1284, 1539, 2402,  194,\n",
       "       5239,   52,   30, 1498,  144,    8,   74,  146,  133,   68,    2,\n",
       "         74,  422,  275,   68,   54,   90,  182,  358,  238,  577,  172,\n",
       "       1634, 1231, 1812, 3341,  119,  291,  578,    2,  703,  208,   13,\n",
       "          3,  224,   47,  124,  341,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(x_tr_text_id[0]))\n",
    "x_tr_text_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2ZwZ1Q2a6Ek",
    "outputId": "08445ce5-8d17-4db4-e2c0-7c5809c82f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful modern new studio in a great location close to the olympic stadium '\n",
      " 'botanical garden nicely decorated it has a really good vibe important long '\n",
      " 'term only short term is not allowed in the building à long terme seulement à '\n",
      " \"court terme n'est pas autorisé dans le bâtiment there is one comfortable \"\n",
      " 'queen bed and one single sofa bed it has high speed internet smart tv 4k '\n",
      " 'elevator sound proof building air conditioning and heating ideal for a '\n",
      " 'couple or two friends']\n"
     ]
    }
   ],
   "source": [
    "#converting the preprocessing sequence back to text\n",
    "pprint(tokenizer.sequences_to_texts(x_tr_text_id[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctzE481ja-Bm",
    "outputId": "191c1707-5842-4456-d5d2-eae8c2c7bc4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words in the dictionary: 40000\n"
     ]
    }
   ],
   "source": [
    "#no. of vocab words\n",
    "print('total words in the dictionary:', tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYiegz2UdMkQ"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt #for plotting \n",
    "import numpy as np\n",
    "import tensorflow as tf   \n",
    "from tensorflow import keras\n",
    "\n",
    "#importing layers\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam #optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wy5552aVbA07"
   },
   "outputs": [],
   "source": [
    "#specifying input layers\n",
    "in_text = keras.Input(batch_shape=(None, max_len))  #for text data input layer shape is (batch, max_length)\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))  #for image data input layer shape (batch, 64,64,2)\n",
    "\n",
    "\n",
    "# text part\n",
    "#embedding-> Each array element is embedded as an array \n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text) #embedding layer Output Shape: (None, 100, 100)\n",
    "averaged = tf.reduce_mean(embedded, axis=1) #averaging embedding layer Output Shape:[(None, 100)]\n",
    "\n",
    "\n",
    "# image part\n",
    "cov = Conv2D(32, (16, 16))(in_image) #2D Convolutional layer with 32 filters and filter size of 16x16, Output: (None, 49, 49, 32)\n",
    "pl = MaxPool2D((16, 16))(cov)        #Maximum pool layer Output Shape: (None, 3, 3, 32)\n",
    "flattened = Flatten()(pl)            #flattening layer Output Shape (None, 288)\n",
    "\n",
    "\n",
    "# fusion:\n",
    "fused = tf.concat([averaged, flattened], axis=-1) #concatenating both image and text data Output Shape: [(None, 388)]\n",
    "\n",
    "# multi-objectives (each is a multi-class classification) final layers\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused) #for price category Output shape: (None, 3)\n",
    "p_type = Dense(len_type, activation='softmax', name='type')(fused)    #for type category Output Shape: (None, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkno2dWygo3U"
   },
   "outputs": [],
   "source": [
    "#Building the model\n",
    "#inputs are text and image data\n",
    "#outputs are price range category and type category\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={                         #specifying inputs\n",
    "        'summary': in_text,\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={                      #specifying outputs\n",
    "        'price': p_price,\n",
    "        'type': p_type,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWUCg4nZg7Ue",
    "outputId": "eb9be657-6e7a-42ff-ec13-16ea36605877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 2)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 49, 49, 32)   16416       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 100)     4000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 3, 3, 32)     0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean (TensorFlowOpL [(None, 100)]        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 288)          0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 388)]        0           tf_op_layer_Mean[0][0]           \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "price (Dense)                   (None, 3)            1167        tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "type (Dense)                    (None, 24)           9336        tf_op_layer_concat[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 4,026,919\n",
      "Trainable params: 4,026,919\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nnDGiRKbG0d"
   },
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "#specifying optimizer loss metrices\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "        'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "        'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "        'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SppT_in5bJfY",
    "outputId": "f732f244-36f0-4dd4-960d-e62fc27c9468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "382/382 [==============================] - 70s 184ms/step - loss: 23.2829 - price_loss: 16.7395 - type_loss: 29.8262 - price_sparse_categorical_accuracy: 0.5040 - type_sparse_categorical_accuracy: 0.5835 - val_loss: 9.5159 - val_price_loss: 5.5714 - val_type_loss: 13.4604 - val_price_sparse_categorical_accuracy: 0.4653 - val_type_sparse_categorical_accuracy: 0.2975\n",
      "Epoch 2/20\n",
      "382/382 [==============================] - 70s 182ms/step - loss: 7.1408 - price_loss: 5.2009 - type_loss: 9.0808 - price_sparse_categorical_accuracy: 0.5224 - type_sparse_categorical_accuracy: 0.5894 - val_loss: 5.4603 - val_price_loss: 4.6177 - val_type_loss: 6.3030 - val_price_sparse_categorical_accuracy: 0.5505 - val_type_sparse_categorical_accuracy: 0.6776\n",
      "Epoch 3/20\n",
      "382/382 [==============================] - 69s 182ms/step - loss: 5.7192 - price_loss: 4.3085 - type_loss: 7.1299 - price_sparse_categorical_accuracy: 0.5371 - type_sparse_categorical_accuracy: 0.6030 - val_loss: 8.8170 - val_price_loss: 5.8190 - val_type_loss: 11.8150 - val_price_sparse_categorical_accuracy: 0.5472 - val_type_sparse_categorical_accuracy: 0.6212\n",
      "Epoch 4/20\n",
      "382/382 [==============================] - 72s 187ms/step - loss: 7.8350 - price_loss: 5.4015 - type_loss: 10.2685 - price_sparse_categorical_accuracy: 0.5391 - type_sparse_categorical_accuracy: 0.5983 - val_loss: 4.0569 - val_price_loss: 3.3223 - val_type_loss: 4.7915 - val_price_sparse_categorical_accuracy: 0.5537 - val_type_sparse_categorical_accuracy: 0.6239\n",
      "Epoch 5/20\n",
      "382/382 [==============================] - 70s 182ms/step - loss: 7.3390 - price_loss: 4.8294 - type_loss: 9.8486 - price_sparse_categorical_accuracy: 0.5694 - type_sparse_categorical_accuracy: 0.6091 - val_loss: 5.1147 - val_price_loss: 4.2732 - val_type_loss: 5.9563 - val_price_sparse_categorical_accuracy: 0.5754 - val_type_sparse_categorical_accuracy: 0.6658\n",
      "Epoch 6/20\n",
      "382/382 [==============================] - 70s 183ms/step - loss: 9.3893 - price_loss: 6.9267 - type_loss: 11.8519 - price_sparse_categorical_accuracy: 0.5591 - type_sparse_categorical_accuracy: 0.6115 - val_loss: 7.4275 - val_price_loss: 5.5328 - val_type_loss: 9.3221 - val_price_sparse_categorical_accuracy: 0.5931 - val_type_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "382/382 [==============================] - 70s 183ms/step - loss: 14.7026 - price_loss: 9.5317 - type_loss: 19.8735 - price_sparse_categorical_accuracy: 0.5770 - type_sparse_categorical_accuracy: 0.6025 - val_loss: 9.2162 - val_price_loss: 6.0237 - val_type_loss: 12.4087 - val_price_sparse_categorical_accuracy: 0.5891 - val_type_sparse_categorical_accuracy: 0.6383\n",
      "Epoch 8/20\n",
      "382/382 [==============================] - 71s 187ms/step - loss: 11.9599 - price_loss: 6.8708 - type_loss: 17.0490 - price_sparse_categorical_accuracy: 0.6050 - type_sparse_categorical_accuracy: 0.6301 - val_loss: 4.8972 - val_price_loss: 3.6814 - val_type_loss: 6.1131 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.5983\n",
      "Epoch 9/20\n",
      "382/382 [==============================] - 70s 184ms/step - loss: 8.8832 - price_loss: 4.5217 - type_loss: 13.2448 - price_sparse_categorical_accuracy: 0.6233 - type_sparse_categorical_accuracy: 0.6294 - val_loss: 7.1263 - val_price_loss: 5.8701 - val_type_loss: 8.3825 - val_price_sparse_categorical_accuracy: 0.4495 - val_type_sparse_categorical_accuracy: 0.6874\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "history = model.fit(\n",
    "    x={\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={\n",
    "        'price': y_tr_price,\n",
    "        'type': y_tr_type,\n",
    "    },\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "            'type': y_vl_type,\n",
    "        }),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, ) #Callback for early stopping\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "fb240405dc214316bbb9f188edd4128b",
      "4aad4725506d4e33ba38ffc549107a91",
      "2f9cc8f33aa342a9987fa67341f00292",
      "9dfc84fbef5b491f8a7a1fc5303a8dcd",
      "73f690e587fb4274a320c5c326228c7a",
      "abbab0fdd0014e9297dddb9e773290e0",
      "c3a4a2404e0f4b508615ca6ab91da8bf",
      "7301a375d8684ce196b3d73b60c1a4af"
     ]
    },
    "id": "_f8tSTgZbNA5",
    "outputId": "1c2a94d3-8f21-426a-f4af-ca5c6d8ddfb5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb240405dc214316bbb9f188edd4128b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7360.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_test_summary = _preprocess(x_test_df.summary.astype(str))\n",
    "x_test_image = np.array([load_image(i) for i in tqdm(x_test_df.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpMS3CFnbPWh",
    "outputId": "08624bfd-aecc-4739-bb78-23493c8031b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3369102e-02 4.4228843e-01 5.3434247e-01]\n",
      " [9.9923074e-01 6.2977883e-04 1.3944837e-04]\n",
      " [9.9322587e-01 6.1475988e-03 6.2647433e-04]\n",
      " ...\n",
      " [2.2346729e-01 7.2570121e-01 5.0831564e-02]\n",
      " [9.9708706e-01 2.5612807e-03 3.5165710e-04]\n",
      " [3.7219042e-01 5.9931797e-01 2.8491611e-02]]\n",
      "[2 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#predicting the values for test data\n",
    "y_predict = model.predict(\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "price_predicted = y_predict['price'] #predicted price\n",
    "print(price_predicted)\n",
    "\n",
    "#converting the array as one number (0,1,2)\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1) #category predictor\n",
    "print(price_category_predicted)\n",
    "\n",
    "#making csv file\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.id,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dppaEjKiyrWj"
   },
   "source": [
    "# Adding Sentence Piece, Bi-GRU, CNN and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyALjgrgnOwb"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt #for plotting \n",
    "import numpy as np\n",
    "import tensorflow as tf   \n",
    "from tensorflow import keras\n",
    "\n",
    "#importing layers\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam #optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wdezy7cU4QQ4"
   },
   "outputs": [],
   "source": [
    "#Text data preprocessing\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  #Tokenizer library to give each word a token \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #To convert tokenized array to a 2D array\n",
    "from sklearn.model_selection import train_test_split  # to make train-validation split\n",
    "from pprint import pprint\n",
    "from keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-v0e-IhYmsqj",
    "outputId": "9cca4360-58f0-4c47-97f8-e3ae46b12624"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roCqFXjQc9VM"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kWuNmlymzaE"
   },
   "outputs": [],
   "source": [
    "#converting all train text + validation text to numpy array\n",
    "text_array = np.array(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZTu36dao9t-"
   },
   "outputs": [],
   "source": [
    "#saving array as .txt file\n",
    "np.savetxt('train_txt.txt',text_array,fmt=\"%s\")\n",
    "#training tokenizer over training text data\n",
    "spm.SentencePieceTrainer.train(input='train_txt.txt', model_prefix='m', vocab_size=14507) \n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file='m.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZybeRhoknv3z"
   },
   "outputs": [],
   "source": [
    "max_len = 200 #maximum length of array for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNmAk2n95r1E",
    "outputId": "f538d4ab-af37-4e59-c408-944efb73b001"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14507"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.vocab_size() #vocabulary size of tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDzTMNrHqWYv",
    "outputId": "cedd1ca0-734a-4050-8cae-dfbedbeb629d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[87, 8, 15, 7, 126, 227]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the tokenizer encoding\n",
    "sp.encode('This is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aM7am6vJqZFZ",
    "outputId": "63e4a448-205d-4ccf-fa1c-313df165e01a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This is a test'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decoding the array\n",
    "sp.decode([87, 8, 15, 7, 126, 227])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HfnpACReC-h"
   },
   "outputs": [],
   "source": [
    "#pad_sequences transforms a list of sequences \n",
    "#into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
    "def _preprocess(list_of_text):\n",
    "    encoded = []                     #empty array to store encoded sentences\n",
    "    for text in list_of_text:        #loop for each sentence array\n",
    "      encoded.append(sp.encode(text))  #encoding the sentence\n",
    "    return pad_sequences(\n",
    "        encoded, \n",
    "        maxlen=max_len,  #maximum length of all sequences\n",
    "        padding='post',  #specifying padding after sequence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfGg47xtflG0",
    "outputId": "b960d04b-2b7e-48fe-ce77-bacf47543615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6101, 200)\n",
      "(1526, 200)\n"
     ]
    }
   ],
   "source": [
    "# padding is done inside: \n",
    "#preprocessing train and validation text data\n",
    "x_tr_text_id = _preprocess(x_tr_text)\n",
    "x_vl_text_id = _preprocess(x_vl_text)\n",
    "\n",
    "#printing shape after preprocessing\n",
    "print(x_tr_text_id.shape)\n",
    "print(x_vl_text_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNPHntRgnTt7",
    "outputId": "1be84611-b656-4a35-d485-96f25ad18879"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 106, 1034, 1737,    7,  849,  184,   82,   38,   10,    7,  175,\n",
       "         24,   39,    7,  144,   44,   20,    6,   36,   25, 2697,  121,\n",
       "       4354,  122,  837,    6,   36,  250,   20,    6,   24,   88,   34,\n",
       "         24,   15, 1472,   39,   10,    6,   79,   61,   11,   78,  148,\n",
       "        395,   53,  298,    3,   70,    3,    5,   30,    4,   87,   18,\n",
       "          9,  246,  132,   36,  299,   44,   20, 1665,   23,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the encoded sequence\n",
    "x_tr_text_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "UjYVEOYvjQip",
    "outputId": "4db86bfb-4ef7-4ce6-b439-8eec7faf62f4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I am renting a huge double bed room in a spacious apartment located a minute away from the metro (you can actually see the metro Laurier from the apartment). The apartment is centrally located in the Plateau Mont-Royal near many cafes, bars, and restaurants. It's only 4 metro stations away from Osheaga! ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \""
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decoding the sequence\n",
    "sp.decode([ 106, 1034, 1737,    7,  849,  184,   82,   38,   10,    7,  175,\n",
    "         24,   39,    7,  144,   44,   20,    6,   36,   25, 2697,  121,\n",
    "       4354,  122,  837,    6,   36,  250,   20,    6,   24,   88,   34,\n",
    "         24,   15, 1472,   39,   10,    6,   79,   61,   11,   78,  148,\n",
    "        395,   53,  298,    3,   70,    3,    5,   30,    4,   87,   18,\n",
    "          9,  246,  132,   36,  299,   44,   20, 1665,   23,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "          0,    0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "RyziFfS2nbXB",
    "outputId": "38db0558-cd13-40c4-c9b5-ca6310444043"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I am renting a huge double bed room in a spacious apartment located a minute away from the metro (you can actually see the metro Laurier from the apartment). The apartment is centrally located in the Plateau Mont-Royal near many cafes, bars, and restaurants. It's only 4 metro stations away from Osheaga!\""
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying the sequence with the encoded one\n",
    "np.array(x_tr_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDINHLDf1GI7"
   },
   "outputs": [],
   "source": [
    "#building the model\n",
    "\n",
    "#specifying input layers\n",
    "in_text = keras.Input(batch_shape=(None, max_len))  #for text data input layer shape is [(None, 200)]\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))  #for image data input layer shape [(None, 64, 64, 2)]\n",
    "# text part\n",
    "#embedding-> Each array element is embedded as an array \n",
    "embedded = keras.layers.Embedding(sp.vocab_size(), 200)(in_text) #embedded layer (None, 200, 200) \n",
    "gru1 = tf.keras.layers.Bidirectional(GRU(150, dropout=0.1,return_sequences=True))(embedded) #Bidirectional with 50 units, Output Shape: (None, 200, 300)  \n",
    "#gru2 = tf.keras.layers.Bidirectional(GRU(75, dropout=0.1,return_sequences=True))(gru1)  #(None,200,100)\n",
    "#fc1 = tf.keras.layers.Dense(150, activation='relu')(gru2)\n",
    "\n",
    "\n",
    "#implementing Attention Mechanism\n",
    "query = Dense(200)(tf.expand_dims(gru1[:, -1, :], [1]))  #query as last output of GRU layer (None, 200, 200)\n",
    "key = Dense(200)(gru1)                    #defining key (None, 200, 200)\n",
    "value = Dense(200)(gru1)                  #defining value (None, 200, 200)\n",
    "\n",
    "att_1 = tf.keras.layers.Attention()([query, value,key])  #implementing attention layer\n",
    "att_1 = tf.squeeze(att_1,[1])                        #removing one dimension \n",
    "\n",
    "# image part\n",
    "drop_1 = tf.keras.layers.Dropout(.1) #dropout layer specification\n",
    "\n",
    "cov1 = Conv2D(45, (2, 2))(in_image) #2D Convolutional layer with 32 filters and filter size of 16x16 Output:(None, 63, 63, 45)\n",
    "outputs = drop_1(cov1, training=True) #(None, 63, 63, 45) \n",
    "p1 = MaxPool2D((3, 3))(outputs) #(None, 21, 21, 45)\n",
    "#cov2 = Conv2D(16, (2, 2))(p1)\n",
    "#p2 = MaxPool2D((2, 2))(cov2)        \n",
    "flattened = Flatten()(p1)            #flattening layer Output Shape (None, 19845) \n",
    "fc2 = tf.keras.layers.Dense(200, activation='relu')(flattened)  #(None, 200)\n",
    "\n",
    "# fusion:\n",
    "\n",
    "#fused = tf.concat([att_1, fc2], axis=-1) #concatenating both image and text data Output Shape: [(None, 388)]\n",
    "fused = tf.reduce_mean([att_1, fc2], axis=0)  #using reduce_mean [(None, 200)]\n",
    "\n",
    "# multi-objectives (each is a multi-class classification) final layers\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused) #for price category Output shape: (None, 3)\n",
    "p_type = Dense(len_type, activation='softmax', name='type')(fused)    #for type category Output Shape: (None, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ua9yZoBr5i01"
   },
   "outputs": [],
   "source": [
    "#Building the model\n",
    "#inputs are text and image data\n",
    "#outputs are price range category and type category\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={                         #specifying inputs\n",
    "        'summary': in_text,\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={                      #specifying outputs\n",
    "        'price': p_price,\n",
    "        'type': p_type,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oX0bOB648zZ",
    "outputId": "eb4816f1-d99f-45c9-c2b0-88ac3f670982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 200)     2901400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 200, 300)     316800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 64, 64, 2)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 300)]        0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 63, 63, 45)   405         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 1, 300)]     0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 63, 63, 45)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 200)       60200       tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200, 200)     60200       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200, 200)     60200       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 21, 21, 45)   0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 1, 200)       0           dense[0][0]                      \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 19845)        0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 200)]        0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          3969200     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean/input (TensorF [(2, None, 200)]     0           tf_op_layer_Squeeze[0][0]        \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean (TensorFlowOpL [(None, 200)]        0           tf_op_layer_Mean/input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "price (Dense)                   (None, 3)            603         tf_op_layer_Mean[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "type (Dense)                    (None, 24)           4824        tf_op_layer_Mean[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 7,373,832\n",
      "Trainable params: 7,373,832\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DWZzOgt5ueF"
   },
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "#specifying optimizer loss metrices\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "        'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "        'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "        'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXYKH6pf5qAb",
    "outputId": "f1160674-3604-492d-c1ec-e6811b334213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "382/382 [==============================] - 244s 638ms/step - loss: 0.5669 - price_loss: 0.5220 - type_loss: 0.6119 - price_sparse_categorical_accuracy: 0.7825 - type_sparse_categorical_accuracy: 0.8253 - val_loss: 0.8360 - val_price_loss: 0.8406 - val_type_loss: 0.8314 - val_price_sparse_categorical_accuracy: 0.6710 - val_type_sparse_categorical_accuracy: 0.7851\n",
      "Epoch 2/7\n",
      "382/382 [==============================] - 244s 639ms/step - loss: 0.4683 - price_loss: 0.4330 - type_loss: 0.5036 - price_sparse_categorical_accuracy: 0.8228 - type_sparse_categorical_accuracy: 0.8559 - val_loss: 0.8860 - val_price_loss: 0.8467 - val_type_loss: 0.9253 - val_price_sparse_categorical_accuracy: 0.6678 - val_type_sparse_categorical_accuracy: 0.7877\n",
      "Epoch 3/7\n",
      "382/382 [==============================] - 244s 639ms/step - loss: 0.3824 - price_loss: 0.3555 - type_loss: 0.4093 - price_sparse_categorical_accuracy: 0.8617 - type_sparse_categorical_accuracy: 0.8825 - val_loss: 1.0652 - val_price_loss: 1.1038 - val_type_loss: 1.0266 - val_price_sparse_categorical_accuracy: 0.6619 - val_type_sparse_categorical_accuracy: 0.7497\n",
      "Epoch 4/7\n",
      "382/382 [==============================] - 245s 641ms/step - loss: 0.3315 - price_loss: 0.3073 - type_loss: 0.3557 - price_sparse_categorical_accuracy: 0.8820 - type_sparse_categorical_accuracy: 0.8949 - val_loss: 1.1004 - val_price_loss: 1.1194 - val_type_loss: 1.0814 - val_price_sparse_categorical_accuracy: 0.6383 - val_type_sparse_categorical_accuracy: 0.7575\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "#Early stopping implemented using restore best weights\n",
    "#which keep the best weights among all the epochs\n",
    "\n",
    "history = model.fit(\n",
    "    x={\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={\n",
    "        'price': y_tr_price,\n",
    "        'type': y_tr_type,\n",
    "    },\n",
    "    epochs=7,\n",
    "    batch_size=16,\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "            'type': y_vl_type,\n",
    "        }),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=3, restore_best_weights=True) #Callback for early stopping\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbsru4E5XJI8"
   },
   "source": [
    "**The above model is the best that has been obtained till yet after fine tuning all the hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "3c5f49a008ec47aba453c8785b99104e",
      "7625ed944bdf4606a2bb926e104b508b",
      "6cf14e0173d24ec3b27c8cd4ba9ce365",
      "41ef2349c9754aa691062c096cda732c",
      "32c83277b42a48038908158cb5e64245",
      "10d37edfb95f4db7a275ab79ff1834f3",
      "102fc7210c7d4ea49b9ae858568d9829",
      "f1835039fdf242c0ac9b7325916d3eb7"
     ]
    },
    "id": "UdEn-sQCDy3r",
    "outputId": "c23fd559-7838-441e-f7bb-31881d4942f8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5f49a008ec47aba453c8785b99104e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7360.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " x_test_summary = _preprocess(x_test_df.summary.astype(str))\n",
    "x_test_image = np.array([load_image(i) for i in tqdm(x_test_df.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rYz0e-yD7aC",
    "outputId": "22a2cd96-e03d-41bc-c631-da3b9b7708fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.8909879e-01 9.8174838e-03 1.0836857e-03]\n",
      " [9.9962616e-01 3.6183812e-04 1.1977868e-05]\n",
      " [9.7420204e-01 2.1012904e-02 4.7850963e-03]\n",
      " ...\n",
      " [7.3074865e-01 2.3168249e-01 3.7568931e-02]\n",
      " [9.9687648e-01 2.8478603e-03 2.7566223e-04]\n",
      " [4.4393906e-01 4.5701787e-01 9.9043071e-02]]\n",
      "[0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "#predicting the values for test data\n",
    "y_predict = model.predict(\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "price_predicted = y_predict['price'] #predicted price\n",
    "print(price_predicted)\n",
    "\n",
    "#converting the array as one number (0,1,2)\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1) #category predictor\n",
    "print(price_category_predicted)\n",
    "\n",
    "#making csv file\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.id,\n",
    "     'price': price_category_predicted}).to_csv('sub15.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXrbIa8Bby7e"
   },
   "source": [
    "# Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRnzRBwlb15S"
   },
   "source": [
    "### Ques 1: What is multi-objective/multi-task learning? What is multi-modality learning? How this assignment fits into those paradigm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG6UO-nKCE4h"
   },
   "source": [
    "Ans: In single objective learning, a specific task is being optimized to get the best model. But in some cases, it becomes important to incorporate related task data to get better results and the method is called as multi-objective/multi-task learning. By sharing information of related task, the model becomes more generalizable and better trained. As in this assignment, the hidden layers are shared between two tasks and the final layer is the task specific layer. This approach is also termed as hard-parameter sharing.\n",
    "\n",
    "**Multi-modality**: Multi-modality traning includes using different kinds of information to train the model. As given in this assignment, the image data and text data information is used to get the required result. The image data information is extracted using Convolutional Neural Network while the text data information is extracted using GRU layer and then finally both the information is combined.\n",
    "\n",
    "In this assignment, we are using multi-task and multi-modality. Here information is combined from two different task (predicting price range and predicting type category) and the two tasks has information in different type: image and text. Thus it is both mutli-task and multi-modality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtTv1r9db7LV"
   },
   "source": [
    "### Ques 2: How attention mechanism can help with the learning process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXQQi7I1ie6S"
   },
   "source": [
    "Ans: Attention Mechanism give us score for each feature given in the data. The score is based upon how important that feature is for the model. By having information about the importance of the features, some heavy weights are assign to that feature in the attention mechanism. This helps in getting more good model as some features (words in this case) impact model more than others. This is very helpful in Natural Language processing as there are many words that carry more information than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyUlQSYCcAuN"
   },
   "source": [
    "### Ques 3: In the template, we use early stopping. What is the purpose? Try to set restore_best_weights to True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLr_7AEj_kNV"
   },
   "source": [
    "Ans: Early Stopping is used to monitor the training of the model. A monitor metric and patience level is specified in the early stopping initialization. Early stopping monitor corresponding metric after each epoch and if the metric values perform badly regularly beyond the patience level, the training is stopped. Thus, early stopping is used to monitor the training and stopping the training if the model is being trained badly.\n",
    "\n",
    "restor_best_weights keeps track of the best model weights. If it is False, only the last epoch weights are used while using it as 'True' makes it use the weights which is best according to the monitor metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGdovqn-cGwS"
   },
   "source": [
    "### Ques 4: Compared to the original tokenizer, what is the advantage of sub-word tokenizers such as sentencepiece ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPNMxzdSF8hZ"
   },
   "source": [
    "Ans: The original tokenizer is based on splitting the sentence into words based on spaces and then tokenizing the sentence based on words splitted away. This is the simplest way of tokenizing the sentences, but this method is not that accurate in many cases. This method fails to incorporate the words that are not exactly same as in the vocabulary. There can be mistakes while writing the text and many other complications which are not incorpoated by given tokenizer.\n",
    "\n",
    "A possible good tokenizer is sub-word tokenizers which is based on that most common words are left as same and the rare words are splitted into meaningful subword units. This method is helpful in many languages other than English. This also enables the model to process words it has never seen before, by decomposing them into subwords it knows. Sentence piece tokenizer is more good to work with languages which don't uses spaces to separate words. Sentence piece also includes the spaces and treat input as raw input and then the appropriate vocabulary is being built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1F_MUp_PGHz"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGyuttVuPKpa"
   },
   "source": [
    "* https://towardsdatascience.com/multimodal-deep-learning-ce7d1d994f4\n",
    "* https://huggingface.co/transformers/tokenizer_summary.html\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "* https://github.com/google/sentencepiece/tree/master/python\n",
    "* https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean\n",
    "* https://ruder.io/multi-task/\n",
    "* https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CISC873_A4_Jagmeet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fc99068670e48eba103a8008afbac1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c477196754ea48a48ef422ce0e5abb3a",
      "placeholder": "​",
      "style": "IPY_MODEL_eb7277c9be9543af81edd38656631c10",
      "value": " 7627/7627 [27:53&lt;00:00,  4.56it/s]"
     }
    },
    "102fc7210c7d4ea49b9ae858568d9829": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10d37edfb95f4db7a275ab79ff1834f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f9cc8f33aa342a9987fa67341f00292": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abbab0fdd0014e9297dddb9e773290e0",
      "max": 7360,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_73f690e587fb4274a320c5c326228c7a",
      "value": 7360
     }
    },
    "32c83277b42a48038908158cb5e64245": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3c5f49a008ec47aba453c8785b99104e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6cf14e0173d24ec3b27c8cd4ba9ce365",
       "IPY_MODEL_41ef2349c9754aa691062c096cda732c"
      ],
      "layout": "IPY_MODEL_7625ed944bdf4606a2bb926e104b508b"
     }
    },
    "41ef2349c9754aa691062c096cda732c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1835039fdf242c0ac9b7325916d3eb7",
      "placeholder": "​",
      "style": "IPY_MODEL_102fc7210c7d4ea49b9ae858568d9829",
      "value": " 7360/7360 [02:44&lt;00:00, 44.72it/s]"
     }
    },
    "43901ea7184b4cc9950514df183fc160": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4aad4725506d4e33ba38ffc549107a91": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5397529f71ff4bcf90b19644d53d9c77": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6cf14e0173d24ec3b27c8cd4ba9ce365": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10d37edfb95f4db7a275ab79ff1834f3",
      "max": 7360,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32c83277b42a48038908158cb5e64245",
      "value": 7360
     }
    },
    "7301a375d8684ce196b3d73b60c1a4af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73f690e587fb4274a320c5c326228c7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7625ed944bdf4606a2bb926e104b508b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9dfc84fbef5b491f8a7a1fc5303a8dcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7301a375d8684ce196b3d73b60c1a4af",
      "placeholder": "​",
      "style": "IPY_MODEL_c3a4a2404e0f4b508615ca6ab91da8bf",
      "value": " 7360/7360 [28:44&lt;00:00,  4.27it/s]"
     }
    },
    "abbab0fdd0014e9297dddb9e773290e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3a4a2404e0f4b508615ca6ab91da8bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c477196754ea48a48ef422ce0e5abb3a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d189ef8fd56143f5b5e5430368e22616": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d674ae5f9e3343a6ac803bee9cbc9ded",
       "IPY_MODEL_0fc99068670e48eba103a8008afbac1f"
      ],
      "layout": "IPY_MODEL_5397529f71ff4bcf90b19644d53d9c77"
     }
    },
    "d674ae5f9e3343a6ac803bee9cbc9ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3f2beaab9674dd183c02371ac4f62c3",
      "max": 7627,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_43901ea7184b4cc9950514df183fc160",
      "value": 7627
     }
    },
    "e3f2beaab9674dd183c02371ac4f62c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb7277c9be9543af81edd38656631c10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1835039fdf242c0ac9b7325916d3eb7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb240405dc214316bbb9f188edd4128b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f9cc8f33aa342a9987fa67341f00292",
       "IPY_MODEL_9dfc84fbef5b491f8a7a1fc5303a8dcd"
      ],
      "layout": "IPY_MODEL_4aad4725506d4e33ba38ffc549107a91"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
